# Task-01: Text Generation with GPT-2

This project demonstrates how to fine-tune OpenAI's GPT-2 model on a custom dataset to generate coherent and contextually relevant text.

## ğŸ” Objective

Train a model to generate text based on a given prompt, mimicking the style and structure of your dataset.

## ğŸ“¦ Dependencies

- Python 3.7+
- transformers
- datasets
- torch
- tqdm

## ğŸ”§ Setup

```bash
pip install -r requirements.txt
```

ğŸ› ï¸ Usage:
1.Place your training data in a .txt file (data.txt).
2.Run the main script:

```bash
python main.py
```
ğŸ“ Output:
Generated text samples will be saved or printed after fine-tuning.

ğŸ“š References:
1.Transformers Docs
2.GPT-2 Paper


