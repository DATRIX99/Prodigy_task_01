# Task-01: Text Generation with GPT-2

This project demonstrates how to fine-tune OpenAI's GPT-2 model on a custom dataset to generate coherent and contextually relevant text.

## 🔍 Objective

Train a model to generate text based on a given prompt, mimicking the style and structure of your dataset.

## 📦 Dependencies

- Python 3.7+
- transformers
- datasets
- torch
- tqdm

## 🔧 Setup

```bash
pip install -r requirements.txt
```

🛠️ Usage:
1.Place your training data in a .txt file (data.txt).
2.Run the main script:

```bash
python main.py
```
📁 Output:
Generated text samples will be saved or printed after fine-tuning.

📚 References:
1.Transformers Docs
2.GPT-2 Paper


